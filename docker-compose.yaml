version: '3.8'

services:
  cluster-agent:
    build:
      context: .
      dockerfile: Dockerfile.dev
    volumes:
      - ./cluster_agent:/agent/cluster_agent
      - cluster-agent-cache:/cache/
    entrypoint: /agent/entrypoint.sh
    environment:
      CLUSTER_AGENT_BASE_SLURMRESTD_URL: http://slurm-master:6820
      CLUSTER_AGENT_DISABLE_SLURM_AUTH: 1
      CLUSTER_AGENT_X_SLURM_USER_NAME: root
      CLUSTER_AGENT_BASE_API_URL: http://jobbergate-api:8000
      CLUSTER_AGENT_SENTRY_DSN:
      CLUSTER_AGENT_AUTH0_DOMAIN: "${AUTH0_DOMAIN}"
      CLUSTER_AGENT_AUTH0_AUDIENCE: "${AUTH0_AUDIENCE}"
      CLUSTER_AGENT_AUTH0_CLIENT_ID: "${AUTH0_CLIENT_ID}"
      CLUSTER_AGENT_AUTH0_CLIENT_SECRET: "${AUTH0_CLIENT_SECRET}"
      CLUSTER_AGENT_CACHE_DIR: /cache
    depends_on:
      jobbergate-api:
        condition: service_healthy

  slurm-master:
    build:
      context: ./etc/docker-slurm
      dockerfile: Dockerfile.master
    entrypoint: /etc/slurm/entrypoint.sh
    environment:
      - SLURM_CPUS_ON_NODE=2
      - DISABLE_UNSHARE_SYSV=true
      - DISABLE_USER_CHECK=true
    ports:
      - 6817:6817
      - 6818:6818
      - 6819:6819
      - 6820:6820
    depends_on:
      slurm-db:
        condition: service_healthy

  slurm-db:
    image: mariadb:latest
    restart: always
    volumes:
      - slurm_db_data:/var/lib/mysql/data/
    environment:
      - MYSQL_ROOT_PASSWORD=slurm-pswd
      - MYSQL_DATABASE=slurm
      - MYSQL_USER=slurm-user
      - MYSQL_PASSWORD=slurm-pswd
    ports:
      - 3306:3306
    healthcheck:
      test: "/usr/bin/mysql --user=slurm-user --password=slurm-pswd --execute \"SHOW DATABASES;\""
      interval: 3s
      timeout: 1s
      retries: 5

  jobbergate-api:
    build:
      context: ../jobbergate/jobbergate-api
      dockerfile: Dockerfile-dev
      args:
        - PYPI_PASSWORD=${PYPI_PASSWORD}
    volumes:
      - ../jobbergate/jobbergate-api/jobbergate_api:/app/jobbergate_api
      - ../jobbergate/jobbergate-api/dev_tools:/app/dev_tools
      - ../jobbergate/jobbergate-api/alembic:/app/alembic
    entrypoint: /app/entrypoint.sh
    environment:
      - DATABASE_HOST=jobbergate-db
      - DATABASE_USER=omnivector
      - DATABASE_PSWD=local-pswd
      - DATABASE_NAME=jobbergate
      - DATABASE_PORT=5432
      - S3_ENDPOINT_URL=http://jobbergate-minio:9000
      - S3_BUCKET_NAME=jobbergate-resources
      - AWS_ACCESS_KEY_ID=FAKE_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY=FAKE_SECRET_ACCESS_KEY
      - ARMASEC_DOMAIN=${AUTH0_DOMAIN}
      - ARMASEC_AUDIENCE=${AUTH0_AUDIENCE}
      - ARMASEC_DEBUG=${ARMASEC_DEBUG}
    ports:
      - 8000:8000
    healthcheck:
      test: curl --fail http://localhost:8000/jobbergate/health || exit 1
      interval: 5s
      retries: 10
      timeout: 5s

  jobbergate-db:
    image: postgres
    restart: always
    volumes:
      - jobbergate_db_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_PASSWORD=local-pswd
      - POSTGRES_USER=omnivector
      - POSTGRES_DB=jobbergate
    ports:
      - 5432:5432

  jobbergate-minio:
    image: minio/minio
    volumes:
      - minio_data:/data
    ports:
      - 9000:9000
      - 9001:9001
    environment:
      - MINIO_ROOT_USER=FAKE_ACCESS_KEY_ID
      - MINIO_ROOT_PASSWORD=FAKE_SECRET_ACCESS_KEY
    command: ["server", "--compat", "--console-address", ':9001', "/data"]

volumes:
  jobbergate_db_data:
  slurm_db_data:
  minio_data:
  cluster-agent-cache:
